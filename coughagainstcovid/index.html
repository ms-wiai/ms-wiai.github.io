
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>mip-NeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <!-- <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf/"/>
    <meta property="og:title" content="mip-NeRF" />
    <meta property="og:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." /> -->

        <!--TWITTER-->
    <!-- <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="mip-NeRF" />
    <meta name="twitter:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf/img/rays_square.png" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Impact of data-splits on generalization : <br> Identifying COVID-19 from cough and context </br> 
                <small>
                     ICLR 2021 : AI for Public Health Workshop
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                          Makkunda Sharma<sup>*</sup>
                        </br>Wadhwani AI
                    </li>
                    <li>
                          Nikhil Shenoy<sup>*</sup>
                        </br>Wadhwani AI
                    </li>
                    <li>
                          Jigar Doshi<sup>*</sup>
                        </br>Wadhwani AI
                    </li><br>
                    <li>
                          Piyush Bagad<sup>*</sup>
                        </br>Wadhwani AI
                    </li>
                    <li>
                          Aman Dalmia
                        </br>Wadhwani AI
                    </li>
                    <li>
                          Rahul Panicker
                        </br>Wadhwani AI
                    </li><br>
                    <li>
                    	<sup>*</sup> Equal Contribution
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2009.08790">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/EpH175PY1A0">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/WadhwaniAI/cough-against-covid">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/teaser-v6.pdf" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
Rapidly scaling screening,  testing and quarantine has shown to be an effectivestrategy to combat the COVID-19 pandemic. We consider the application of deep learning techniques to distinguish individuals with COVID from non-COVID byusing data acquireable from a phone.  Using cough and context (symptoms andmeta-data) represent such a promising approach.  Several independent works inthis direction have shown promising results. However, none of them report perfor-mance across clinically relevant data-splits.  Specifically, the performance where the development and test sets are split in time (retrospective validation) and across sites (broad validation). Although there is meaningful generalization across these splits the performance significantly varies (up to 0.1 AUC score). In addition, we study the performance on symptomatic and asymptomatic individualsacross these three splits.  Finally, we show that our model focuses on meaningful features of the input, ‘cough’ bouts for cough and relevant symptoms for context.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Our Proposed Solution
                </h3>
                <p style="text-align:center;">
                    <image src="img/arch_v4.pdf" height="50px" class="img-responsive">
                </p>
                <p class="text-justify">
                    We develop a CNN-based framework that ingests spectrogram representations of audio and directly predicts the probability of the presence of COVID-19.
                    <br>
                     For our context-based classification task, we leverage TabNet as our classifier. 
                     <br>
                     For every individual, we get predicted probabilities p(y= 1|xcough) and p(y=1|xcontext) from the individual classifiers. For our final prediction we use a simple  ensembling scheme that averages the predictions from the two classifiers.
                </p>
                <!-- <p style="text-align:center;">
                    <image src="img/ipe_eqn_under_pad.png" height="30px" class="img-responsive">
                </p>
                <video id="v0" width="100%" autoplay loop muted>
                  <source src="img/ipe_anim_horiz.mp4" type="video/mp4" />
                </video>
                <p class="text-justify">
                    We can see that when considering a wider region, the higher frequency features automatically shrink toward zero, providing the network with lower-frequency inputs. As the region narrows, these features converge to the original positional encoding.
                </p> -->
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Our Data
                </h3>
                <p class="text-justify">
                    We collect this dataset from individuals who have undergone a COVID-19 test, from numerous testing sites across the country. In addition, contextual data such as symptoms,travel history, contact with confirmed case and demographic information etc is collected .  Unlike crowd-sourced datasets that rely on self-reported COVID-19 status, our ground-truth is lab test results from the healthcarefacilities.  This dataset consists of 12,780 cough sounds from 4,260 individuals from 27 differentsites. 1,394 have a positive test result and 2866 remaining are tested negatives.
                </p>
                <h4>
                	Our Data Statistics
                </h4>
                <p style="text-align:center;">
                    <image src="img/combined_demographics_v3.pdf" class="img-responsive" alt="scales">
                </p>
                <h4>
                	Our Data Splitting Strategy
                </h4>
                <p class="text-justify">
                    In order to establish robustness towards deployment of our model,  we use 3 different strategies for splitting the data.   All samples of an individual are always part of the same set .The details on the data distribution across the splits are shown in the figure above .
                </p>
                <p style="text-align:center;">
                    <image src="img/data_slicing_grid_v3.pdf" class="img-responsive" alt="scales">
                </p>
                <p style="text-align:center;">
                    <image src="img/data-dist-v2.pdf" class="img-responsive" alt="scales">
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Data Visualization
                </h3>
                <p class="text-justify">
                    Given the clinical uncertainty of this task and the use of deep learning for it, it is essential for clinicians to qualitatively understand the model behaviour and its predictions.  As a sanity check, we employ GradCAM++ to compute these saliency map.  We consistently observe that the focus-areas are on and around the cough bouts.  This reinforces the belief that the model is indeed making predictions based on coughsignal and otherwise.
                    <br>
                    For the context-based classifier, we use Local Interpretable Model-agnostic Explanations (LIME) to understand which specific features help the model differentiate between COVID+ and COVID- patients at an instance level.  
                </p>                
                <br>
                <p style="text-align:center;">
                    <image src="img/gradcam-random-1911.pdf" height="50px" class="img-responsive">
                </p>
                <p style="text-align:center;">
                    <image src="img/gradcam-random-3581.pdf" height="50px" class="img-responsive">
                </p>
                <p style="text-align:center;">
                    <image src="img/gradcam-random-2263.pdf" height="50px" class="img-responsive">
                </p>
                <p style="text-align:center;">
                    <image src="img/gradcam-random-1217.pdf" height="50px" class="img-responsive">
                </p>
                <br><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.
                </p>                
                <br>
                <br><br>
                <p class="text-justify">
                    We can also manipulate the integrated positional encoding by using a larger or smaller radius than the true pixel footprint, exposing the continuous level of detail learned within a single network:
                </p>     
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    How to access Data
                </h3>
                To access the data , download the files given below with the relevant splits . Contact us at <a href= "mailto:coughagainstcovid@wadhwaniai.org"> coughagainstcovid@wadhwaniai.org </a> for the password .
                <br>
                <br>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">Spectrogram : </a> This contains the spectrograms of the the audio data present . 
                </p>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">Wav2vec : </a> This contains data passed through wav2vec2 .
                </p>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">VGGish : </a> This contains data passed through VGGish .
                </p>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">OpenSMIL : </a> This contains data features passed through openSMIL .
                </p>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">Resnet : </a> This contains output when the spectrogram is passed through our trained resnet 18 model .
                </p>
            </div>
        </div>
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@misc{bagad2020cough,
      title={Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds}, 
      author={Piyush Bagad and Aman Dalmia and Jigar Doshi and Arsha Nagrani and Parag Bhamare and Amrita Mahale and Saurabh Rane and Neeraj Agarwal and Rahul Panicker},
      year={2020},
      eprint={2009.08790},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We are thankful to AWS for covering the entire cost of the GPUs used for this work .
                    <br>
                 We also thank James Zhou (Stanford), and Peter Small) and Puneet Dewan (Global Health  Labs) for very helpful discussions, inputs, and evangelism. We are grateful to Ankit Baghel, Anoop Manjunath and Arda Sahiner from Stanford for helping with curating the cough pre-training dataset .
                    <br>
                    We also want to thank the Governments of Bihar and Odisha, and the Municipal Corporation of Greater Mumbai for extending necessary approvals and facilitating activities for data collection in respective geographies.
                    <br>
                    We are grateful to Ashfaq Bhat and his team at Norway India Partnership Initiative for supporting data collection and advocacy efforts in the state of Bihar and Odisha. Ravikant Singh and his team at Doctors for You for playing a critical role in initiating data collection, getting IRB approvals and managing field operations. Pankaj Bhardwaj and Suman Saurabh from Department of Community Medicine in All India Institute of Medical sciences, Jodhpur for leading the data collection efforts in the institute.
                    <br>
                    We greatly appreciate the support of our lovely team members at Wadhwani AI. Nikhil Velpanur played a key role helping early data collection, supported by Akshita Bhanjdeo and Patanjali Pahwa. Puskar Pandey has been helping ensure continued data collection. Bhavin Vadera provided important support for data collection in additional sites. Vishal Agarwal helped build essential digital tools for data collection. Kalyani Shastry managed the entire logistics and coordinated supplies needed for field data collection at various study sites.
                    <br>
                    And finally, we are humbled by the passion, hard work, and dedication of our numerous field staff. They have ensured strict adherence of the safety protocols through the data collection effort while maintaining high data quality.
                    <br>
                    <br>
                    <br>
                The website template was borrowed from <a href="https://jonbarron.info/mipnerf/">Jon Barron</a> who took it from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
